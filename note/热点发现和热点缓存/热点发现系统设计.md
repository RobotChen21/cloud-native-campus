# 📘 高并发架构专题：热点发现系统 (Hot Key Discovery)

## 1. 核心背景：为什么要发现热点？

- **定义：** 某个 Key 的访问量远远超过其他 Key（例如：某明星出轨、秒杀商品ID）。
- **危害：**
  1. **流量集中：** 单个 Redis 分片被打爆（网卡打满/CPU 100%）。
  2. **缓存击穿：** 热点失效瞬间，无数请求打穿到 DB，导致数据库雪崩。
  3. **影响邻居：** 因为 Redis 是单线程/集群分片，一个热点 Key 会拖慢该分片上所有普通 Key 的响应。
- **根本解法：** **将热点 Key 迁移到应用服务器的“本地内存”（Local Cache），切断对 Redis 的访问。**

------

## 2. 架构演进之路 (Architecture Evolution)

面试时不要上来就丢出大厂方案，要按照这个顺序讲，体现深度。

### Level 1: 原始阶段 - 本地粗糙计数

最简单的做法，开发直接在 Java 代码里写死。

- **实现：** 用一个 `HashMap<String, AtomicLong>` 记录最近访问的 Key。
- **缺点：**
  1. **内存泄漏风险：** Key 太多把 JVM 撑爆。
  2. **视野局限：** 只能看到本机流量，无法发现全局热点。比如 100 台机器，每台访问 10 次，单机看不是热点，但全局 1000 次就是热点。
- **结论：** 玩具级方案，生产环境慎用。

### Level 2: 进阶阶段 - 旁路大数据分析 (Flink/Spark)

利用公司现有的日志系统进行分析。

- **流程：** 应用层打印日志 -> Logstash 收集 -> Kafka -> **Flink 实时计算** -> 算出热点 -> 推送 Redis/Nacos -> 业务层订阅。
- **优点：** 统计极其精准，可以做排行榜。
- **缺点（致命伤）：** **延迟太高！**
  - 整个链路跑完通常需要 5秒~1分钟。
  - **痛点：** 对于“秒杀”或“突发新闻”，前 5 秒数据库已经挂了，Flink 算出来也没用了。
- **结论：** 适合做 BI 报表、热搜榜单，**不适合做系统保命**。

模糊化处理（适合想秀一下架构视野，但不想被深问）

话术策略：把 Flink 和 Logstash 称为“大数据计算服务”和“日志采集组件”，强调数据流向，而不是组件细节。

面试模拟：
面试官：你们的热点发现是怎么做的？

你：我们参考了业界通用的“流式计算”方案。
1.  首先，应用层会异步收集用户的访问日志，投递到 Kafka 中（这里重点强调 Kafka 削峰）。
2.  然后，后端有一个独立的计算服务（这里别主动提 Flink，就说是消费者服务），订阅 Kafka 的消息。
3.  这个计算服务内部维护了一个时间窗口（比如 5 秒），实时统计商品的访问频率。
4.  一旦发现某个商品访问量超过阈值，就把它判定为“热点”，然后推送到 Nacos 配置中心。
5.  各业务微服务监听到 Nacos 变动，自动把这个热点商品缓存到 JVM 本地内存（Caffeine） 中，不再查
    Redis。

如果面试官追问：“你那个计算服务用什么写的？Flink 吗？”
你（诚实作答）：“在阿里、字节这种大厂通常是用 Flink
做实时计算。但我们项目考虑到运维成本和团队技术栈，目前是用 Java 服务直接消费 Kafka，在内存里用 Map
做的一个简易滑动窗口统计。虽然性能不如 Flink
极致，但对于我们目前的量级完全够用了，而且不需要维护额外的大数据组件。”

### Level 3: 大厂/终极阶段 - 客户端上报 + 实时计算节点 (Client-Server 模式)

这是目前业界（京东 JD-hotkey、阿里 Sentinel）的主流做法。

- **核心思想：** 不全量上报日志，否则网络会风暴。只上报“可疑分子”；不走 HTTP，走长连接。
- **架构组件：**
  1. **Client (SDK)：** 嵌入在 Java 业务代码中。
  2. **Worker (计算集群)：** 专门负责聚合计算热点的 Netty Server。
  3. **Etcd/Zookeeper：** 负责存储规则和元数据。
- **流程：**
  1. **预筛选：** Client 发现某个 Key 1秒内访问 > 5 次，标记为“可疑”，通过 Netty 异步发给 Worker。
  2. **聚合：** Worker 收到 100 台机器发来的“可疑 weibo_888”，一加发现 > 500 次。为了防止计算节点挂掉，按 Key 的 Hash 值分发。
  3. **下发：** Worker 确认是热点，立刻通过长连接把 `weibo_888` 推送给所有 Client。
  4. **传递：** 抛弃 HTTP，使用 Netty (TCP/NIO) 进行长连接通信。使用 ProtoBuf(Protocol Buffers) 序列化，数据包尽量小。
  5. **生效：** Client 收到后，设置一个短的过期时间（如 10 秒）。将该 Key 放入 Caffeine 本地缓存。
  6. **失效：** Worker 持续探测，如果热度降下来了，不发送“续期”指令，Client 端的本地缓存自然过期，恢复正常 Redis 访问。
- **优势：** **毫秒级延迟**（通常 < 500ms），资源消耗极低。

### 架构对比表

| **维度**     | **大数据方案 (Kafka + Flink)** | **轻量级方案 (SDK + Netty)**   |
| ------------ | ------------------------------ | ------------------------------ |
| **定位**     | 全局统计、排行榜、BI 分析      | **系统保护、实时防御**         |
| **时效性**   | 秒级/分钟级 (慢)               | **毫秒级 (快)**                |
| **准确性**   | 精确 (全量数据)                | 近似 (可能采样/TopN)           |
| **资源消耗** | 极大 (Hadoop集群、大量磁盘IO)  | 极小 (内存计算、Netty长连接)   |
| **适用场景** | 微博热搜榜、抖音热门视频榜     | **秒杀、突发新闻、防缓存击穿** |

------

## 3. 面试实战话术 (Interview Script)

当面试官问：**“你们是怎么处理热点 Key 的？”** 或者 **“如何设计一个热点发现系统？”**

请按照以下 **STAR (Situation, Task, Action, Result)** 逻辑回答：

### 第一步：抛出痛点 (Situation)

> “在之前做类似微博/秒杀的项目时，我们发现一个核心问题：**二八定律**非常明显。绝大多数请求都集中在极少数的几个 Key 上（比如头部大V的动态）。
>
> 如果完全依赖 Redis，这些热点 Key 很容易把单分片的网卡打满，甚至如果缓存击穿，数据库瞬间就会崩掉。”

### 第二步：否定过渡方案 (Thinking Process) - *这里是加分项*

> “起初我们想过用 **Flink** 做实时日志分析。但经过评估发现不合适，因为 Flink 的链路太长（收集->Kafka->计算），延迟通常在秒级。
>
> **对于高并发防御来说，延迟就是生命。** 等 Flink 算出热点，我们的数据库可能早就挂了。我们需要一个**毫秒级**的发现方案。”

### 第三步：提出最终方案 (Action) - *亮出“大厂标准”*

> “所以，我们参考了业界成熟的方案（如京东的 JD-hotkey），设计了一套**轻量级的热点探测机制**。核心分为三层：
>
> 1. **客户端 SDK：** 在业务代码的 Filter 层做**滑动窗口预统计**。如果某个 Key 在单机频次超过阈值（比如 5QPS），我们不认为是热点，但认为是‘嫌疑人’，通过 **Netty 长连接** 异步上报给计算节点。
> 2. **计算服务端 (Worker)：** 这是一组无状态的 Netty Server。它们接收所有客户端报上来的‘嫌疑人’，进行**全局聚合**。如果总量超过全局阈值（比如 1000 QPS），就判定为真正热点。
> 3. **反向推送：** 一旦确认为热点，Worker 立刻把这个 Key 推送回所有的业务客户端。
>
> 客户端收到推送后，会把这个 Key 放入 JVM 的 **Caffeine 本地缓存**，并设置一个短的过期时间（如 10秒）。”

### 第四步：总结效果 (Result)

> “通过这套系统，我们将热点发现的延迟控制在 **500ms 以内**。
>
> 当热点出现时，99% 的流量直接在 JVM 本地被消化，完全不经过 Redis 和数据库，极大地保护了后端存储。”

------

## 4. 关键技术细节 (Q&A 备用)

面试官可能会追问细节，你要准备好以下“重型武器”：

#### Q1: 为什么用 Netty 不用 HTTP？

- **答：** HTTP 协议头太重，且每次都要三次握手。热点上报是非常高频的操作，必须用 **TCP 长连接 + NIO**（Netty），配合 Protobuf 序列化，让数据包极其轻量，才能扛住洪峰。

#### Q2: 既然本地缓存了，数据更新了怎么办？（一致性问题）

- **答：** 这是一个 Trade-off（权衡）。
  - 对于热点数据，我们通常接受**“最终一致性”**。
  - 方案 A：本地缓存设置极短的 TTL（比如 3秒）。用户最多看到 3 秒前的旧数据，完全可接受。
  - 方案 B：数据更新时，通过 MQ 广播通知所有机器失效本地缓存（架构复杂，不推荐）。
  - **通常回答“短 TTL”即可。**

#### Q3: 所有的 Key 都上报吗？那样带宽会不会爆？

- **答：** 绝对不行。必须采用**“写时采样”**或**“阈值上报”**。
  - 只有单机访问频率超过一定值（如 5次/秒）的 Key，才有资格被上报。
  - 绝大多数冷门 Key 根本不会触发上报逻辑，所以带宽占用非常小。

------

## 5. 极简伪代码 (用于手写代码环节)

如果面试官让你写核心逻辑，不要写 Netty 通信，写**业务侧的接入逻辑**。

Java

```
// 核心业务逻辑
public WeiboContent getWeibo(String weiboId) {
    
    // 1. 【热点探测 & 降级】
    // 这一步由 SDK 自动完成：
    // a. 统计访问量
    // b. 异步上报
    // c. 检查是否在 worker 推送过来的 "LocalHotKeyMap" 中
    if (HotKeySdk.isHotKey(weiboId)) {
        // 如果是热点，直接读本地 Caffeine/Guava
        WeiboContent localData = LocalCache.get(weiboId);
        if (localData != null) return localData;
    }

    // 2. 【标准 Redis 流程】
    // 不是热点，或者热点刚形成本地还没数据
    String key = "weibo:" + weiboId;
    WeiboContent data = redis.get(key);
    
    // 3. 【上报逻辑 (伪代码示意)】
    // 实际上这一步是异步做的，不会阻塞主线程
    HotKeySdk.collect(weiboId); 

    if (data == null) {
         // ... 执行标准的缓存击穿防御 (互斥锁查 DB) ...
         data = db.query(weiboId);
         redis.set(key, data);
    }
    
    return data;
}
```
------

## 6. 补充：核心概念通俗理解 (大白话版)

为了方便理解“流式计算”架构，可以将各个组件做如下类比：

1.  **应用层日志 (Logs) = 路口摄像头**
    *   **作用**：负责记录谁经过了（被访问了）。没有它，系统就是瞎子。
2.  **Logstash = 快递员**
    *   **作用**：负责去每个服务器（路口）收录像带（日志），打包发走。
3.  **Kafka = 蓄水池 / 缓冲仓库**
    *   **作用**：**削峰填谷**。当海量日志涌入时，先存在这里，防止下游被冲垮。
4.  **Flink = 实时指挥官 / 超级大脑**
    *   **作用**：**实时计算**。在内存中开窗口数数（如：5秒内谁出现了1000次？），发现热点后大喊一声。
5.  **本地缓存 (Caffeine) = 贴身保镖**
    *   **作用**：一旦收到指挥官的“热点通知”，立刻启动保护模式，不再让请求打到后端数据库。

## 7. 面试避坑指南 (重要!)

*   **原则**：**简历没写、自己没做过、原理不深究的技术，绝对不要在面试中作为“重头戏”去讲。**
*   **策略**：
    *   如果你不熟悉 Flink/Logstash，**不要主动展开讲 Level 2 的细节**。
    *   **推荐话术**：采用文中 **Level 3 (JD-HotKey)** 的思路，或者 **Level 2 的“降级版”**（即：Java 消费者消费 Kafka -> 内存 HashMap 统计）。
    *   **核心理由**：强调我们选择了**“轻量级、低运维成本、纯 Java 技术栈”**的方案，这体现了架构师的**务实**精神（Cost-Control）。


