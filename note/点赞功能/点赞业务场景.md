#### 业务场景：用户在视频评论区进行高频操作：首先对 **Top 级热评（30w+ 赞）** 进行点赞，随后通过**快速滑动**（瀑布流加载）定位并点赞 **长尾（冷门）评论**。在切换视频后立即返回，要求点赞状态（亮色图标）必须实现毫秒级回显，且点赞计数需准确同步。用户访问 **一年前的历史视频**，评论区需同时加载并回显该用户在过去对某热评及某冷评的点赞记录（状态位检索）。随后，用户对该旧视频下的其他热评发起 **增量点赞**。针对上述场景，本分析将忽略视频业务逻辑，聚焦于**评论点赞功能**。

我们把这两个业务场景拆解为：

1. **计数业务（Counting）**：评论有多少赞（聚合维度：`comment_id`）基于热度 (LRU/TTL)。
2. **状态业务（Status）**：我有没有点赞（聚合维度：`user_id`）基于时间 (Timeline)。

我会按照你描述的用户行为轨迹，一步步拆解后台发生了什么，以及最后如何做的持久化。

------

### 第一阶段：当下热点交互（Redis 热数据主场）

#### 场景 1：用户给“热门评论（30w赞）”点赞

> **用户行为**：点赞了一个大热评。大热评肯定在Redis中，所以只靠读。

- **业务 A：计数（Counting）**
  - **Redis 操作**：`INCR comment:likes:count:{hot_comment_id}`。
  - **特点**：因为是热门评论，并发极高。为了防止 Redis 热 Key 问题，大厂通常会在应用层（Service 内存）做一个微小的 **Local Buffer**（比如 100ms 聚合一次），然后一把 `INCRBY` 加到 Redis 上。
- **业务 B：状态（Status）**
  - **Redis 操作**：`ZADD user:likes:{uid} {timestamp} {hot_comment_id}`。
  - **解释**：将这个评论 ID 加入该用户的“点赞时间线” ZSet 中。Score 是当前时间戳，保证最新的在最前。
- **持久化（Persistence）**
  - **动作**：点赞成功的瞬间，发送一条 **MQ 消息** `{"uid": 1, "cid": 888, "action": "add"}`。
  - **落地**：
    1. **状态库**：消费者收到消息，插入 `user_like_relation` 表（分库分表）。
    2. **计数库**：消费者收到消息，更新 `comment_stats` 表（`count = count + 1`）。

#### 场景 2：向下滑动，给“冷门评论”点赞

> **用户行为**：快速滑动，随机点赞了一个只有 2 个赞的评论。查看一堆评论的点赞数。评论流混合冷热评论，所以Redis不一定存在。

- **业务 A：计数（Counting）**
- **读取逻辑（Read-Through）—— 用户浏览冷门的评论视频**：
    - **Redis Miss**：发现是冷数据。
    - **回源 + 回填**：查 DB 得到基数 -> **写入 Redis**（设置 **短 TTL**，如 10 分钟，冷数据的TTL变得极短就可以）。
    - **目的**：看防止瞬间流量（Cache Breakdown）击穿数据库。如果该评论持续有人，TTL 会自动续期，实现“冷变热”的平滑过渡。
    - **Redis Hit**：直接读取数量即可。
- **写入逻辑（Write-Through）用户点赞了冷门的评论视频**：
    - **特点**：冷数据，Redis 可能没有这个 Key（过期或未缓存）。
    - **Redis Miss**：加锁 -> 查 DB -> 回写 Redis -> `INCR`。（注意缓存击穿）
    - **Redis Hit**：直接 `INCR comment:likes:count:{cold_comment_id}，**续期 TTL**。
- **持久化**：发送 MQ 异步落库，消费者合并写库。

- **业务 B：状态（Status）**
  - **Redis 操作**：`ZADD user:likes:{uid} {timestamp} {cold_comment_id}`。
  - **解释**：同样的操作。用户的 ZSet 里现在多了一条记录。
- **持久化**：同上，发 MQ 异步落库。

#### 场景 3：脑抽滑回来，重看评论区（Read Your Own Write）

> **用户行为**：滑走又回来，看到刚才点的赞亮着。刚才已经存到Redis中了，所以都在Redis中。

- **前端请求**：拉取当前屏幕的评论列表（比如 10 条）。

- **后端处理**：

  1. **取计数**：`MGET comment:likes:count:{id1, id2...}`。从 Redis 批量拿点赞数。
  2. **取状态**：
     - 调用 `ZRANGE user:likes:{uid}` 获取用户最近的 N 条点赞记录（比如最近 100 条）。
     - **内存比对**：后端服务在内存中判断：当前屏幕的 10 个评论 ID，是否在用户的 ZSet 列表里？
     - **结果**：刚才点的热门评论和冷门评论 ID 都在 ZSet 里，匹配成功 -> **返回 `is_liked: true`**。

  - **核心价值**：因为刚才写入的是 Redis ZSet（热数据），读取也是读 Redis，速度极快，实现了“所见即所得”。

------

### 第二阶段：考古交互（冷热分离与兜底）

#### 场景 4：打开一年前的视频，发现“已点赞”的评论，给老视频的另一个热门评论点赞（老树发新芽）

> **用户行为**：在老视频里又产生了一个新点赞。比如神评翻红，《甄嬛传》再次爆火
- **业务 A：计数（Counting）—— 针对“复热”场景的防御**
- **读取逻辑（Read-Through）—— 用户浏览旧的评时评**：
    - **Redis Miss**：发现是冷数据。
    - **回源 + 回填**：查 DB 得到基数 -> **写入 Redis**（设置 **短 TTL**，如 10 分钟，冷数据的TTL变得极短就可以）。
    - **目的**：看防止瞬间流量（Cache Breakdown）击穿数据库。如果该评论持续有人，TTL 会自动续期，实现“冷变热”的平滑过渡。

- **写入逻辑（Write-Through）用户点赞了旧的评论视频**：
    - **Redis Miss**：加锁 -> 查 DB -> 回写 Redis -> `INCR`。（注意缓存击穿）
    - **Redis Hit**：直接 `INCR`，**续期 TTL**。
- **持久化**：发送 MQ 异步落库，消费者合并写库。

- **业务 B：状态（Status）**
- **写入逻辑（Write-Through）用户点赞了旧的评论视频**：
  - **Redis 操作**：`ZADD user:likes:{uid} {now} {new_hot_comment_id}`。
  - **效果**：虽然视频是老的，但**操作是新的**。这条数据会被放到 ZSet 的**最顶端**。
  - **用户视角**：下次再刷新，这条记录直接在 Redis 命中，不需要走布隆和 DB。
- **持久化**：照常发 MQ 落库。

- **读取逻辑（Read-Fallback）用户查看一年前的历史视频评论**：
    - **Redis 操作**：`ZRANGE user:likes:{uid}` 获取热数据列表（取出用户最近的 1000 条点赞），发现 ID 不在列表中（因数据过旧已被 LRU/TTL 淘汰）。根据“水位线法则”，要查的评论比Redis最旧的点赞记录都老，所以不确定Redis是否存在。必须要三步走。
    - **兜底机制**：利用 布隆过滤器 (Bloom Filter) 进行快速初筛。数据库中是否存在用户点赞记录。布隆过滤器可以每一个用户一个（对comment_id进行哈希），也可以维护一个超大的布隆过滤器（对user_id+comment_id进行哈希）。
      - 若布隆说“无”：直接判定未点赞（100% 准确，无需查库）。虽然布隆过滤器对取消点赞无效，但是取消点赞是低频行为，漏一两个去查数据库也无所谓。
      - 若布隆说“有”（存在误判率）：才发起 SQL 回源查询 `SELECT count(*) FROM user_like_relation WHERE uid=? AND comment_id IN (hot_old, cold_old)` 进行最终确认。
      - 记住查完也不写回Redis，我们的Redis存的都是最新的点赞，不能破坏 ZSet 的“时间窗口”逻辑，防止缓存污染与淘汰冲突。
    - **性能核心**：布隆过滤器成功拦截了 99% 的无效“未点赞”查询，防止了冷数据查询引发的数据库穿透。

****

### 刷评论场景: “水位线法则”图解

假设：

1. **用户 A** 的 Redis ZSet 里存了他最近的点赞记录。
2. ZSet 里**最老**的一条记录时间是 `2024-01-01`（这就是**水位线 / Min_Score**）。
3. 这就意味着：**2024-01-01 之后的所有点赞，Redis 里全都有；之前的，Redis 里没有。**

现在，Feed 流刷出了两个视频，我们在内存里做判断：

#### 情况 A：刷到一个“新视频/新评论”

- **视频发布时间**：`2024-06-01`。
- **判断逻辑**：
  1. 这个视频比我的水位线（2024-01-01）要**新**。
  2. **推论**：**如果**我点赞过它，点赞时间肯定晚于 `2024-06-01`。
  3. 既然 `2024-06-01` 肯定在 Redis 的覆盖范围内（`> 2024-01-01`），而 Redis 列表里却**没有**这个 ID。
  4. **结论**：**铁定没赞过！**（直接返回 False，**无需**查布隆/DB）。

#### 情况 B：刷到一个“考古视频/老评论”

- **视频发布时间**：`2023-05-01`。
- **判断逻辑**：
  1. 这个视频比我的水位线（2024-01-01）要**老**。
  2. **推论**：我完全可能在 2023 年赞过它，但那个记录因为太老，已经被移出 Redis 了。
  3. **现状**：Redis 里没这个 ID，但我无法确定是因为“没赞过”还是“被清理了”。
  4. **结论**：**进入“三步走”流程**（查布隆 -> 查 DB）。

------

####  极简代码逻辑 (伪代码)

这段代码你可以记一下，面试时候写出来非常加分：

Java

```
// 1. 一次性获取用户的热点赞列表 (Redis IO 仅 1 次)
Set<Long> likedIds = redisService.zrevrange("user:likes:1001", 0, 1000);

// 2. 获取这个列表里最老的一条数据的时间 (水位线)
// 如果列表为空，或者没满1000条，说明该用户所有历史都在 Redis 里，水位线 = 0 (远古时期)
long minCachedTime = redisService.getMinScore("user:likes:1001"); 

// 3. 遍历 Feed 流里的内容 (纯内存运算)
for (Comment comment : commentList) {
    // 【第一关】直接命中：Redis 里有，那就是赞了
    if (likedIds.contains(comment.getId())) {
        comment.setIsLiked(true);
        continue;
    }

    // 【第二关】时间截断：Redis 里没有，且内容很新 -> 肯定没赞
    // 逻辑：内容发布时间 > 缓存最老时间
    if (comment.getPublishTime() > minCachedTime) {
        comment.setIsLiked(false); 
        continue;
    }

    // 【第三关】考古兜底：Redis 没有，且内容很老 -> 不确定，查到底
    // 只有这种情况才走布隆过滤器/数据库
    boolean realStatus = dbService.checkLikeStatus(userId, comment.getId());
    comment.setIsLiked(realStatus);
}
```

#### 4. 总结

你的理解**“点赞时间晚于内容发布时间”**是这个算法的物理基石（因果律）。

由此推导出的**“安全区”**逻辑是：

> 只要 **内容发布时间** > **Redis缓存的最早时间**，
>
> Redis 里查不到，就等于**绝对没点赞**。

只有在这个安全区之外（远古内容），我们才需要动用“三步走”（Redis -> Bloom -> DB）这个重武器。这样就完美解释了为什么 Feed 流这么快，而不用每次都查库。

------

### 总结：两大业务的架构与持久化对比

| **维度**       | **1. 计数业务 (Counting)**                                                                    | **2. 状态业务 (Status)**                                                                 |
|--------------|-------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| **如何解决**     | 基于热度 (LRU/TTL)，比如老剧，老评论翻红。                                                                | 基于时间 (Timeline)，feed流基本都是最新的视频评论。                                                    |
| **核心问题**     | 这是一个“公共数据”，所有人看到的数字都一样。                                                                   | 这是一个“私有数据”，每个人看到的都不一样。                                                               |
| **Redis 结构** | **String** (K-V) Key: `comment:count:{id}` Value: `300000`                                | **ZSet** (Sorted Set) Key: `user:likes:{uid}` Value: `comment_id` Score: `timestamp` |
| **读操作**      | `GET` 命中直接返回。Miss 则查 DB（基数），并回写 Redis (设置短 TTL)。 核心：防止冷视频突发翻红击穿数据库。                                    | **Timeline 模式** 先查 Redis ZSet (热) -> 再查布隆 (防穿透) -> 最后查 DB (冷)。                       |
| **写操作**      | **高并发缓冲** 热评：Local Buffer -> Redis INCR。 冷评：直接 Redis INCR。                                | **写最新** 直接 `ZADD` 放入 Redis ZSet 头部，保证最新操作立即可见。                                       |
| **持久化策略**    | **异步聚合更新** MQ 消费者攒一波（比如 +5），然后 `UPDATE comment SET count = count + 5`。 这叫“写合并”，减少 DB 行锁竞争。 | **流水记录** MQ 消费者单条（或批量）插入 `INSERT IGNORE INTO user_like_relation`。 这是数据的**根基**。       |
| **主要瓶颈**     | **写瓶颈**：热门评论瞬间被 10 万人点赞，DB 扛不住更新。                                                         | **读瓶颈**：Feed 流每页都要查“我赞过没”，QPS 极高；ZSet 太大会导致 BigKey。                                  |
| **解决方案**     | Redis 抗读，MQ+聚合写 抗写。                                                                       | Redis 只存热数据 (Timeline)，布隆+DB 兜底冷数据。                                                  |

**简单一句话总结这个过程：**
“评论内容（不可变/低频变）”和“点赞数（高频变）”存在同一个 Key（比如一个大 JSON）里，那就是架构设计上的“自杀行为”，一般要进行分离。所以本章只讨论点赞数和点赞状态。
你在滑动的过程中，**Redis ZSet** 像一个“短期记忆背包”，记录了你刚才所有的操作，让你回头看时能秒开；而 **MySQL** 像一个“永久档案室”，通过 MQ 默默地把你所有的痕迹归档。当你去翻一年前的“老档案”时，系统通过布隆过滤器这把“快速索引枪”，精准地帮你从档案室里把记录调了出来。