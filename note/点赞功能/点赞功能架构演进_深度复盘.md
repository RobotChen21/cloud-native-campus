# 天机学堂 - 点赞功能架构演进与深度复盘

> **核心摘要**：本文档记录了点赞业务从基础需求分析，到发现传统方案瓶颈，再到参考大厂架构进行深度优化的全过程。重点解决了高并发下的**BigKey问题**、**Feed流查询性能问题**以及**数据库写压力问题**。同时深入探讨了冷热数据分离后的**数据一致性兜底策略**。

---

## 1. 业务理解与痛点分析

### 1.1 核心业务场景
在“天机学堂”及主流内容平台（抖音、B站）中，点赞业务主要包含两个核心视角：
1.  **内容视角 (Content-Centric)**：
    *   **需求**：展示视频/文章的**点赞总数**。
    *   **特点**：高频读（所有人都看），高频写（热门内容）。
    *   **热度规律**：与发布时间无关，只与当前热度有关（如老剧翻红）。
2.  **用户视角 (User-Centric) - Feed 流痛点**：
    *   **需求**：在刷 Feed 流列表时（一页 20 条），图标需要展示“**我是否已点赞**”。
    *   **特点**：高频只读查询，要求极致低延迟。
    *   **热度规律**：强时间相关，用户绝大多数时间只关注**最新**内容的状态。

### 1.2 原始方案的痛点
如果简单使用 MySQL 存储 `user_like_relation` 表：
*   **读瓶颈**：Feed 流查询时，需要对 20 条内容发起 `SELECT count(*) ... WHERE uid=? AND biz_id IN (...)`，即使有索引，分库分表后也难以高效聚合。
*   **写瓶颈**：热门事件瞬间几百万 QPS 写入，MySQL 直接宕机。

---

## 2. 老师的教学方案 (V1.0 - 基础版)

> **方案特点**：结构清晰，强一致性，适合中小规模并发。

### 2.1 数据结构
1.  **去重集合 (Redis Set)**：
    *   Key: `likes:set:{bizId}` (业务ID)
    *   Value: `userId`
    *   *作用*：利用 Set 天然去重，判断用户是否重复点赞。
2.  **排行榜/计数 (Redis ZSet)**：
    *   Key: `likes:top:{bizType}`
    *   Member: `bizId`，Score: `count` (点赞数)
    *   *作用*：支持排行榜查询，作为写缓冲。

### 2.2 流程逻辑
1.  **用户点赞** -> `SADD` 存入 Set -> `SCARD` 统计当前 Set 大小 -> `ZADD` 覆盖 ZSet 的 Score。
2.  **异步落库** -> 定时任务轮询 ZSet，批量取出 (Pop) 发生变化的数据 -> 发送 MQ -> 更新 MySQL。

### 2.3 存在的缺陷 (面试降维打击点)
1.  **BigKey 隐患**：若某课程点赞超百万，`likes:set:{bizId}` 会变成巨型 Set，导致 Redis 读写阻塞、集群迁移困难。
2.  **Feed 流性能差**：
    *   查询“我是否点赞”时，必须按 `bizId` 维度去查 Set。
    *   刷 20 条内容，后端需要循环调用 20 次 Redis (`sismember`)。网络 RTT 消耗巨大。
3.  **内存浪费**：存全量 `userId` 对于业务价值极低（不需要展示全量名单），浪费 90% 内存。
4.  **机制漏洞**：单纯 `ZPOPMIN` 按点赞数排序会导致高赞内容**饥饿 (Starvation)**，永远无法落库。

---

## 3. 我们的深度思考与质疑 (Critical Thinking)

### 3.1 质疑一：能不能不发 MQ，直接攒在 Redis 定时写库？
*   **推演**：利用 Redis ZSet 做写缓冲，定时任务轮询落库。
*   **结论**：**不可行 (Anti-Pattern)**。
    1.  **性能灾难**：千万级用户 Key (`user:likes:uid`)，定时任务无法高效 `SCAN` 遍历，会拖死 Redis。
    2.  **状态复杂**：难以区分“哪些已落库，哪些未落库”，需维护复杂的状态标记。
    3.  **数据风险**：Redis 挂了数据即丢。MQ 才是**可靠持久化**的唯一通道。
*   **修正**：**“攒”的动作应下沉到 MQ 消费者端**。利用 MQ 做事件驱动，消费者内部 Buffer 攒够 500ms 批量写 DB。

### 3.2 质疑二：点赞总数怎么同步给业务服务？
*   **方案**：**Cache-Aside (旁路缓存) + TTL 自动续期**。
    *   **Redis**：`likes:count:{bizId}` 设置 7 天过期。
    *   **策略**：每次读/写该 Key，重置过期时间。
    *   **效果**：热点数据（无论新旧）常驻内存；冷数据自动淘汰，有人挖坟时查库回填。

---

## 4. 终极架构方案 (V2.0 - 大厂标准版)

> **核心哲学**：读写分离、**差异化冷热分离**、维度解耦。

### 4.1 核心数据结构设计

#### A. 解决“计数” (Global Count) - 基于热度 (LRU/TTL)
*   **结构**：Redis String
*   **Key**：`likes:count:{bizId}`
*   **Value**：纯数字
*   **操作**：
    *   **写**：`INCR` + `EXPIRE 7天`。超级热点使用**本地内存聚合**后异步写 Redis。
    *   **读**：`GET` + `EXPIRE 7天`。Cache Miss 则回源查 DB。
*   **持久化**：发送 MQ -> 消费者聚合更新 `content_stats` 表。

#### B. 解决“状态” (User Status) - 基于时间 (Timeline)
*   **结构**：Redis ZSet (**User-Centric**)
*   **Key**：`user:likes:{userId}`
*   **Value**：`bizId`
*   **Score**：`timestamp` (点赞时间)
*   **操作**：
    *   **写**：`ZADD`。
    *   **读**：Feed 流查询时，`ZRANGE` 取出该用户**最近 1000 条**点赞，在内存中比对。
*   **冷热分离**：
    *   **热 (Redis)**：ZSet 仅保留最近 3 个月数据（或最新的 1000 条）。
    *   **冷 (Bloom/DB)**：Feed 流绝大多数是新内容，直接命中 Redis。若遇到远古内容 Redis 查不到，进入“三大流派”处理逻辑（详见下文）。

#### C. 关系持久化 (Relationship Persistence) - 全量底座
> *这是数据安全的最后一道防线，用于支撑“考古”查询和数据恢复。*

*   **表结构**：`user_like_relation`
    *   `id`: **本地自增主键 (Local Auto-Inc)**。
        *   *策略*：在分库分表后，允许每个分片表的 `id` 重复。
        *   *理由*：业务查询完全基于 `user_id` 分片键，**永远不使用 `id` 进行查询**。避免引入全局 ID 生成器的性能开销。
    *   `user_id` (Sharding Key): **分片键**。
    *   `biz_id`: 业务ID。
    *   `created_at`: 点赞时间。
*   **唯一索引**：`UNIQUE KEY(user_id, biz_id)` —— **强制约束幂等性**，防止 MQ 重复投递导致脏数据。
*   **分库分表策略**：
    *   **必须按 `user_id` 分片**。
    *   **理由**：我们的查询场景通常是“查用户 A 的点赞记录”。如果按 `user_id` 分片，查询只需要落到一个数据库节点；如果按 `biz_id` 分片，查询用户 A 的记录时就需要全库扫描（广播），性能不可接受。

### 4.2 架构全景流 (Life of a Like)

**场景**：用户刷 Feed 流点赞了一个视频。

1.  **写流程 (Write)**：
    *   **APP** -> **API**。
    *   **Step 1 (状态)**：`ZADD user:likes:uid now videoId` (更新用户时间线)。
    *   **Step 2 (计数)**：本地 Buffer 聚合 -> `INCRBY likes:count:videoId` (更新总数)。
    *   **Step 3 (持久化)**：**重点！** 发送 MQ 消息 `{"uid":..., "vid":..., "op":"ADD"}`。
        *   *注意：这里不需要等 Redis 攒够，而是动作发生即发送（或本地聚合后发送）。*
    *   **Step 4 (落库)**：MQ 消费者收到消息 -> 放入本地 Buffer -> 攒够 50 条 ->
        *   `INSERT IGNORE INTO user_like_relation ...` (按 userId 分片入库)。
        *   `UPDATE content_stats SET count = count + N` (聚合写)。

2.  **读流程 (Read)**：
    *   **Feed 服务** 获取 20 个 videoId。
    *   **并行查询**：
        *   **查数字**：`MGET likes:count:ids` (热点自动续期)。
        *   **查状态**：`ZRANGE user:likes:uid` (拿回最近列表) -> 内存比对 `contains(videoId)`。
    *   **Step 3 (考古)**：
        *   若 Feed 流中出现了 1 年前的视频，且 Redis ZSet 里没查到（过期了）。
        *   参见下文 **“5. 进阶探讨”**。

### 4.3 核心释疑：数据如何随时间推移持久化？(Data Lifecycle)

**用户的疑问**：Redis 里的 `user:likes` 只存最近 3 个月，那 3 个月后数据去哪了？是 Redis 过期的时候才写数据库吗？

**架构师解答**：
**绝对不是！持久化动作发生在“点赞的那一刻”，而不是“过期的那一刻”。**

1.  **出生即永生 (Born to Persistence)**：
    *   当用户点赞时，系统并发做了两件事：
        *   **动作 A**：写 Redis ZSet（为了让用户立马能刷到，热数据）。
        *   **动作 B**：发 MQ 消息（为了持久化到 MySQL）。
    *   **结果**：所以在你点赞后的 **1 秒钟内**，MySQL 数据库里**已经**有了这条记录。

2.  **自然衰退 (Aging)**：
    *   Redis ZSet 里的数据会随着时间推移（如超过 3 个月），被定时任务或 ZSet 的容量限制策略（如 `ZREMRANGEBYRANK` 保留前 1000 条）**静默移除**。
    *   **移除就移除了**，不需要做任何“搬运”操作。因为 MySQL 里早就有一份全量的备份了。

---

## 5. 进阶探讨：冷热数据一致性的三大流派

> **问题背景**：当用户 Redis ZSet 中的数据因过期或超出 1000 条限制被剔除后（冷数据），用户在 Feed 流刷到该“远古视频”时，Redis 查不到记录，此时如何处理？

### 5.1 方案 A：认怂策略 (Acceptable Inconsistency) —— 大多数 App 的选择
*   **策略逻辑**：
    *   **Feed 流列表页**：如果 Redis 查不到，直接**默认未点赞**（图标灰色）。
    *   **详情页**：只有当用户真正点进去时，才发起一次**回源查询**（查 MySQL），获取确切状态。
*   **理由**：
    *   Feed 流是“过客”，用户刷得很快，对 1 年前的某个视频的点赞状态不敏感。
    *   为了那 1% 的“远古老数据”去查数据库，导致拖慢 99% 的“新数据”查询速度（因为 DB 慢），性价比极低。
*   **补救措施**：
    *   如果用户误以为没点赞，又点了一次？
    *   后端接口做**幂等性处理**：发现 MySQL 里已经有了 -> 返回“点赞成功” -> **顺便把这个记录写回 Redis 热数据** -> 前端图标变亮。用户无感知，体验顺畅。

### 5.2 方案 B：客户端本地存储 —— 端侧优化
*   **策略逻辑**：
    *   APP 本地缓存用户的点赞历史（使用 SQLite 或 Realm）。
    *   服务端只下发新产生的点赞数据。
*   **适用场景**：对流量极度敏感，且拥有强大客户端研发团队的公司。

### 5.3 方案 C：布隆过滤器 (Bloom Filter) 兜底 —— 技术流的选择
> 利用布隆过滤器“说没有就一定没有”的特性，精准拦截冷数据查询。

*   **实现方式**：
    *   Redis 查不到时 -> 查布隆过滤器。
    *   布隆说“没有” -> 返回 `false`（绝对准确）。
    *   布隆说“有”（可能是误判） -> **回源查 MySQL** 确认。

*   **深度思考：现实性与副作用**
    1.  **内存现实性**：给每个用户建一个布隆是不现实的（Key 太多，元数据开销大）。大厂通常使用 **RedisBloom 插件维护全局分片布隆**，将 `userId:bizId` 作为元素存入。
    2.  **副作用（不支持删除）**：布隆过滤器的原生特性是不支持删除。
        *   *场景*：用户点赞后又“取消点赞”，Redis 和 DB 删了，但布隆删不掉。
        *   *后果*：用户再次刷到该视频，布隆返回“有”，程序去查 DB，DB 说“没有”，最终返回 `false`。这导致了**回源穿透**。
    3.  **架构权衡**：
        *   在业务上，“点赞后又取消”是极低频行为（<1%）。
        *   相比于布隆拦截掉的 99% “从未点赞”的无效流量（保护了 DB），容忍这 <1% 的因取消点赞导致的 DB 穿透，是**性价比极高**的决策。

---

## 6. 扩展思考：亿级流量下的架构突变 (ByteDance Scale)

> *当系统演进到抖音/TikTok 级别（日活 8 亿+，千亿级数据量）时，MySQL 分库分表也将遭遇瓶颈。*

### 6.1 存储选型的根本性转变 (LSM-Tree)
*   **MySQL 的局限**：B+ 树在海量随机写入下，分裂与平衡带来的磁盘 IO 开销过大，难以支撑百万级 TPS 写入。
*   **NoSQL 上位**：采用 **HBase / TiKV / Cassandra** 等基于 **LSM-Tree (Log-Structured Merge-Tree)** 的分布式数据库。
    *   **原理**：将随机写转换为**顺序追加写 (Append Only)**，写入性能提升 10~100 倍。
    *   **优势**：天生分布式，扩容只需加机器，无数据迁移痛苦。

### 6.2 避免“写热点” (Hotspot Prevention)
*   **MySQL 分库分表 (Hash 分片)**：
    *   通常采用 `userId % N` 分片。
    *   即使使用**本地自增主键**，只要查询不依赖主键，且分片键是随机散列的 `userId`，**不存在写热点问题**。数据会均匀写入各个库。
*   **HBase/TiKV (Range 分片)**：
    *   大厂常用这些库支持范围查询。
    *   如果 RowKey 具有**单调递增**特性（如直接用雪花算法生成的 `userId` 或时间戳），所有新数据都会落在集群的**最后一个节点 (Region)**，导致严重的单点过热。
    *   **大厂解法**：抛弃单纯的顺序 ID，RowKey 设计采用 **`Hash(userId) + Timestamp`** 或 **`Reverse(userId)`** 来强制打散写入压力。

### 6.3 社交图谱 (Graph DB)
*   **需求升级**：不仅仅查“我赞了谁”，还要查“我的朋友里谁赞了这个视频”（社交推荐）。
*   **技术栈**：引入分布式图数据库（如 **ByteGraph** / **Neo4j**），存储复杂的多对多关系，支撑二度人脉查询。

---

## 7. 面试急救包：如果面试官追问 LSM-Tree？

**Q: 既然你提到了 HBase，为什么它比 MySQL 写得快？**

**A: (三点核心)**
1.  **MySQL (B+树)**：写入时需要不断在树中寻找位置、分裂节点，产生大量**随机磁盘 IO**。即使是自增主键，维护二级索引也会带来随机写。
2.  **HBase (LSM-Tree)**：核心是**追加写 (Append Only)**。所有新数据直接写到内存和文件末尾，像写日志一样，将随机 IO 转换为**顺序 IO**，速度提升几十倍。
3.  **分布式热点 (区分场景)**：
    *   **MySQL Hash 分片**：无热点，哪怕用自增 ID 也没事（只要不按 ID 查）。
    *   **HBase Range 分片**：**怕递增**。所以必须设计 Hash 或反转的 RowKey。
